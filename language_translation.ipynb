{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data import TranslationDataset\n",
    "from rnn import RNN, RNNTools\n",
    "from transformers import Transformers, TransformersTools\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurable parameters, change as needed\n",
    "\n",
    "# set to true if loading existing model file, false if training a new model\n",
    "skip_training = True\n",
    "data_dir = 'data'\n",
    "rnn_model_save_path = 'models/rnn.pth'\n",
    "tra_model_save_path = 'models/transformers.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs if not existing\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device type: cpu\n"
     ]
    }
   ],
   "source": [
    "# additional settings, automatically selects cuda if available\n",
    "if skip_training:\n",
    "    device_type = 'cpu'\n",
    "elif torch.cuda.is_available():\n",
    "    device_type = 'cuda:0'\n",
    "else:\n",
    "    device_type = 'cpu'\n",
    "\n",
    "# set manually if needed e.g. device_type = 'cpu'\n",
    "print(\"Using device type:\", device_type)\n",
    "device = torch.device(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence pairs in the training set:  8682\n",
      "Number of sentence pairs in the test set:  2171\n"
     ]
    }
   ],
   "source": [
    "trainset = TranslationDataset(data_dir, train=True)\n",
    "testset = TranslationDataset(data_dir, train=False)\n",
    "print('Number of sentence pairs in the training set: ', len(trainset))\n",
    "print('Number of sentence pairs in the test set: ', len(testset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=RNNTools.collate, pin_memory=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=False, collate_fn=RNNTools.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4489, 256)\n",
       "    (gru): GRU(256, 256)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2925, 256)\n",
       "    (gru): GRU(256, 256)\n",
       "    (out): Linear(in_features=256, out_features=2925, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(trainset.input_lang.n_words, trainset.output_lang.n_words, embed_size=256, hidden_size=256)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    PADDING_VALUE = 0 \n",
    "    teacher_forcing_ratio = 0.5\n",
    "    num_epochs = 2\n",
    "\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)    \n",
    "    criterion = nn.NLLLoss(ignore_index=PADDING_VALUE)\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_data = 0\n",
    "        for src_seqs, src_seq_lengths, tgt_seqs in trainloader:\n",
    "            src_seqs, tgt_seqs = src_seqs.to(device), tgt_seqs.to(device)\n",
    "            \n",
    "            if torch.rand(1) < teacher_forcing_ratio:\n",
    "                teacher_forcing=True\n",
    "            else:\n",
    "                teacher_forcing=False\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = rnn(src_seqs, tgt_seqs, src_seq_lengths, teacher_forcing)\n",
    "            loss = criterion(outputs.permute(0, 2, 1).to(device), tgt_seqs)\n",
    "            \n",
    "            # compute loss metric\n",
    "            total_loss += (loss.item() * src_seqs.shape[1])\n",
    "            total_data += src_seqs.shape[1]\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch: {0} training loss: {1:.3f}\".format(epoch, total_loss/total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    torch.save(rnn.state_dict(), rnn_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model loaded from: models/rnn.pth\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    rnn.load_state_dict(torch.load(rnn_model_save_path, map_location=lambda storage, loc: storage))\n",
    "    print('RNN model loaded from: {}'.format(rnn_model_save_path))\n",
    "    rnn.to(device)\n",
    "    rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnntools = RNNTools(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index=range(20), columns=['batch_i', 'Source', 'Actual Translation', 'RNN Translation', 'Transformer Translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | Source                                | Actual Translation               | RNN Translation                        |\n",
       "|---:|:--------------------------------------|:---------------------------------|:---------------------------------------|\n",
       "|  0 | vous vous etes trompe d avion .       | you are on the wrong plane .     | you re always anticipating of others . |\n",
       "|  1 | je suis interesse par l anglais .     | i am interested in english .     | i m interested in english .            |\n",
       "|  2 | nous sommes tellement fiers de vous ! | we re so proud of you !          | we re so proud of you !                |\n",
       "|  3 | tu mens n est ce pas ?                | you re lying aren t you ?        | you re staying aren t you ?            |\n",
       "|  4 | il est un peu emeche .                | he s a bit tipsy .               | he s a bit rough .                     |\n",
       "|  5 | elle sembla indifferente .            | she seemed uninterested .        | she seemed uninterested .              |\n",
       "|  6 | tu es tout seul .                     | you re all alone .               | you re all alone .                     |\n",
       "|  7 | je ne suis pas une menteuse .         | i m not a liar .                 | i m not a liar .                       |\n",
       "|  8 | je suis hongrois .                    | i am hungarian .                 | i m a .                                |\n",
       "|  9 | nous avons juste peur .               | we re just scared .              | i m still waiting .                    |\n",
       "| 10 | desole d etre si stupide .            | i m sorry i m so stupid .        | i m so happy for your loss . .         |\n",
       "| 11 | nous sommes une grande famille .      | we re a big family .             | we re a big boy .                      |\n",
       "| 12 | je ne vais pas citer de noms .        | i m not going to name names .    | i m not going to lose .                |\n",
       "| 13 | elles sont toutes parties .           | they re all gone .               | they re all dead .                     |\n",
       "| 14 | je suis amoureuse de toi .            | i m in love with you .           | i m on your side .                     |\n",
       "| 15 | elle l a sermonne .                   | she scolded him .                | she studies him .                      |\n",
       "| 16 | je suis dans le jardin .              | i am in the garden .             | i am in the bathtub .                  |\n",
       "| 17 | il profite de sa vie d ecolier .      | he is enjoying his school life . | he is afraid to have his hair .        |\n",
       "| 18 | vous n etes pas si vieux tom .        | you re not that old tom .        | you re not that old tom .              |\n",
       "| 19 | vous n etes pas millionnaire .        | you re not a millionaire .       | you re not a millionaire .             |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test data:')\n",
    "\n",
    "i = 0\n",
    "for src_seqs, src_mask, tgt_seqs in testloader:\n",
    "    if i >= 20:\n",
    "        break\n",
    "\n",
    "    out_seqs = rnntools.translate(rnn, src_seqs, src_mask)\n",
    "\n",
    "    for r in random.sample(range(0, 64), 1):\n",
    "        results_df.loc[i, 'batch_i'] = r\n",
    "        results_df.loc[i, 'Source'] = rnntools.seq_to_string(src_seqs[:,r], testset.input_lang)\n",
    "        results_df.loc[i, 'Actual Translation'] = rnntools.seq_to_string(tgt_seqs[:,r], testset.output_lang)\n",
    "        results_df.loc[i, 'RNN Translation'] = rnntools.seq_to_string(out_seqs[:,r], testset.output_lang)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "display(Markdown(results_df[['Source', 'Actual Translation', 'RNN Translation']].to_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 96.69817090034485\n",
      "BLEU score on test data: 47.73730933666229\n"
     ]
    }
   ],
   "source": [
    "score = rnntools.compute_bleu_score(rnn, trainloader, trainset.output_lang)\n",
    "print(f'BLEU score on training data: {score*100}')\n",
    "score = rnntools.compute_bleu_score(rnn, testloader, trainset.output_lang)\n",
    "print(f'BLEU score on test data: {score*100}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=TransformersTools.collate, pin_memory=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=False, collate_fn=TransformersTools.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformers(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4489, 256, padding_idx=0)\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-2): 3 x EncoderBlock(\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2925, 256, padding_idx=0)\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-2): 3 x DecoderBlock(\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=2925, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra = Transformers(trainset.input_lang.n_words, trainset.output_lang.n_words, n_blocks=3, n_features=256, n_heads=16, n_hidden=1024)\n",
    "tra.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    PADDING_VALUE = 0\n",
    "    num_epochs = 2\n",
    "\n",
    "    optimizer = torch.optim.Adam(tra.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss(ignore_index=PADDING_VALUE)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_data = 0\n",
    "        for src_seqs, src_mask, tgt_seqs in trainloader:\n",
    "            src_seqs, src_mask, tgt_seqs = src_seqs.to(device), src_mask.to(device), tgt_seqs.to(device)\n",
    "            \n",
    "            # forward\n",
    "            outputs = tra(src_seqs, tgt_seqs, src_mask)\n",
    "            \n",
    "            # compute loss metric\n",
    "            loss = criterion(outputs.permute(0, 2, 1).to(device), tgt_seqs[1:])\n",
    "            total_loss += (loss.item() * src_seqs.shape[1])\n",
    "            total_data += src_seqs.shape[1]\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch: {0} training loss: {1:.3f}\".format(epoch, total_loss/total_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    torch.save(tra.state_dict(), tra_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers model loaded from: models/transformers.pth\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    tra.load_state_dict(torch.load(tra_model_save_path, map_location=lambda storage, loc: storage))\n",
    "    print('Transformers model loaded from: {}'.format(tra_model_save_path))\n",
    "    tra.to(device)\n",
    "    tra.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tratools = TransformersTools(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "20\n",
      "19\n",
      "25\n",
      "20\n",
      "34\n",
      "61\n",
      "41\n",
      "13\n",
      "49\n",
      "39\n",
      "43\n",
      "33\n",
      "9\n",
      "50\n",
      "26\n",
      "38\n",
      "34\n",
      "4\n",
      "9\n",
      "34\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | Source                                | Actual Translation               | Transformer Translation          |\n",
       "|---:|:--------------------------------------|:---------------------------------|:---------------------------------|\n",
       "|  0 | vous vous etes trompe d avion .       | you are on the wrong plane .     | she is devoted to her children . |\n",
       "|  1 | je suis interesse par l anglais .     | i am interested in english .     | she s too loud .                 |\n",
       "|  2 | nous sommes tellement fiers de vous ! | we re so proud of you !          | he s a fit .                     |\n",
       "|  3 | tu mens n est ce pas ?                | you re lying aren t you ?        | you re very intelligent .        |\n",
       "|  4 | il est un peu emeche .                | he s a bit tipsy .               | he is my hero .                  |\n",
       "|  5 | elle sembla indifferente .            | she seemed uninterested .        | i m from croatia .               |\n",
       "|  6 | tu es tout seul .                     | you re all alone .               | she s six years older than me .  |\n",
       "|  7 | je ne suis pas une menteuse .         | i m not a liar .                 | i m not your husband anymore .   |\n",
       "|  8 | je suis hongrois .                    | i am hungarian .                 | we re being attacked .           |\n",
       "|  9 | nous avons juste peur .               | we re just scared .              | i m undressing .                 |\n",
       "| 10 | desole d etre si stupide .            | i m sorry i m so stupid .        | you re alone aren t you ?        |\n",
       "| 11 | nous sommes une grande famille .      | we re a big family .             | i m ready for them .             |\n",
       "| 12 | je ne vais pas citer de noms .        | i m not going to name names .    | she is war .                     |\n",
       "| 13 | elles sont toutes parties .           | they re all gone .               | i m not sure .                   |\n",
       "| 14 | je suis amoureuse de toi .            | i m in love with you .           | we re all ears .                 |\n",
       "| 15 | elle l a sermonne .                   | she scolded him .                | i m truly sorry .                |\n",
       "| 16 | je suis dans le jardin .              | i am in the garden .             | they are christians .            |\n",
       "| 17 | il profite de sa vie d ecolier .      | he is enjoying his school life . | he is eight .                    |\n",
       "| 18 | vous n etes pas si vieux tom .        | you re not that old tom .        | he is able to play his entire .  |\n",
       "| 19 | vous n etes pas millionnaire .        | you re not a millionaire .       | i m as tall as my grandfather    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Test data:')\n",
    "\n",
    "i = 0\n",
    "for src_seqs, src_mask, tgt_seqs in testloader:\n",
    "    if i >= 20:\n",
    "        break\n",
    "\n",
    "    out_seqs = tratools.translate(tra, src_seqs, src_mask)\n",
    "\n",
    "    r = results_df.loc[i, 'batch_i']\n",
    "    print(r)\n",
    "    results_df.loc[i, 'Transformer Translation'] = tratools.seq_to_string(out_seqs[:,r], testset.output_lang)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "display(Markdown(results_df[['Source', 'Actual Translation', 'Transformer Translation']].to_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 95.63669647179238\n",
      "BLEU score on test data: 58.79185315508608\n"
     ]
    }
   ],
   "source": [
    "score = tratools.compute_bleu_score(tra, trainloader, trainset.output_lang)\n",
    "print(f'BLEU score on training data: {score*100}')\n",
    "score = tratools.compute_bleu_score(tra, testloader, trainset.output_lang)\n",
    "print(f'BLEU score on test data: {score*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Source                                | Actual Translation               | RNN Translation                        | Transformer Translation          |\n",
       "|---:|:--------------------------------------|:---------------------------------|:---------------------------------------|:---------------------------------|\n",
       "|  0 | vous vous etes trompe d avion .       | you are on the wrong plane .     | you re always anticipating of others . | she is devoted to her children . |\n",
       "|  1 | je suis interesse par l anglais .     | i am interested in english .     | i m interested in english .            | she s too loud .                 |\n",
       "|  2 | nous sommes tellement fiers de vous ! | we re so proud of you !          | we re so proud of you !                | he s a fit .                     |\n",
       "|  3 | tu mens n est ce pas ?                | you re lying aren t you ?        | you re staying aren t you ?            | you re very intelligent .        |\n",
       "|  4 | il est un peu emeche .                | he s a bit tipsy .               | he s a bit rough .                     | he is my hero .                  |\n",
       "|  5 | elle sembla indifferente .            | she seemed uninterested .        | she seemed uninterested .              | i m from croatia .               |\n",
       "|  6 | tu es tout seul .                     | you re all alone .               | you re all alone .                     | she s six years older than me .  |\n",
       "|  7 | je ne suis pas une menteuse .         | i m not a liar .                 | i m not a liar .                       | i m not your husband anymore .   |\n",
       "|  8 | je suis hongrois .                    | i am hungarian .                 | i m a .                                | we re being attacked .           |\n",
       "|  9 | nous avons juste peur .               | we re just scared .              | i m still waiting .                    | i m undressing .                 |\n",
       "| 10 | desole d etre si stupide .            | i m sorry i m so stupid .        | i m so happy for your loss . .         | you re alone aren t you ?        |\n",
       "| 11 | nous sommes une grande famille .      | we re a big family .             | we re a big boy .                      | i m ready for them .             |\n",
       "| 12 | je ne vais pas citer de noms .        | i m not going to name names .    | i m not going to lose .                | she is war .                     |\n",
       "| 13 | elles sont toutes parties .           | they re all gone .               | they re all dead .                     | i m not sure .                   |\n",
       "| 14 | je suis amoureuse de toi .            | i m in love with you .           | i m on your side .                     | we re all ears .                 |\n",
       "| 15 | elle l a sermonne .                   | she scolded him .                | she studies him .                      | i m truly sorry .                |\n",
       "| 16 | je suis dans le jardin .              | i am in the garden .             | i am in the bathtub .                  | they are christians .            |\n",
       "| 17 | il profite de sa vie d ecolier .      | he is enjoying his school life . | he is afraid to have his hair .        | he is eight .                    |\n",
       "| 18 | vous n etes pas si vieux tom .        | you re not that old tom .        | you re not that old tom .              | he is able to play his entire .  |\n",
       "| 19 | vous n etes pas millionnaire .        | you re not a millionaire .       | you re not a millionaire .             | i m as tall as my grandfather    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_df[['Source', 'Actual Translation', 'RNN Translation', 'Transformer Translation']].to_markdown()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_projs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
