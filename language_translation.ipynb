{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French to English Language Translation with RNN and Transformers\n",
    "\n",
    "The notebook provides training for two language translation models using the RNN and Transformers algorithms. The architecture for the models can be found in their respective files, ```rnn.py``` and ```transformers.py```. Additionally, the data pre-processing code can be found in ```data.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data import TranslationDataset\n",
    "from rnn import RNN, RNNTools\n",
    "from transformers import Transformers, TransformersTools\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're retraining the model, set ```skip_training``` to ```True```. \n",
    "Newly trained models will be saved as ```models/{architecture}.pth```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurable parameters, change as needed\n",
    "\n",
    "# set to true if loading existing model file, false if training a new model\n",
    "skip_training = True\n",
    "data_dir = 'data'\n",
    "rnn_model_save_path = 'models/rnn.pth'\n",
    "tra_model_save_path = 'models/transformers.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs if not existing\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device_type is automatically set to ```cuda``` if it's available; otherwise, it's set to ```cpu```. You can also manually overwrite it if you have a different device setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device type: cpu\n"
     ]
    }
   ],
   "source": [
    "# additional settings, automatically selects cuda if available\n",
    "if skip_training:\n",
    "    device_type = 'cpu'\n",
    "elif torch.cuda.is_available():\n",
    "    device_type = 'cuda:0'\n",
    "else:\n",
    "    device_type = 'cpu'\n",
    "\n",
    "# set manually if needed e.g. device_type = 'cpu'\n",
    "print(\"Using device type:\", device_type)\n",
    "device = torch.device(device_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and preprocess. \n",
    "\n",
    "Preprocessing includes tasks such as tokenization, where each sentence is split into individual words or subword units, and mapping each word or subword unit to an index value. This mapping creates a dictionary, which is used to convert the sentences into sequences of index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence pairs in the training set:  8682\n",
      "Number of sentence pairs in the test set:  2171\n"
     ]
    }
   ],
   "source": [
    "trainset = TranslationDataset(data_dir, train=True)\n",
    "testset = TranslationDataset(data_dir, train=False)\n",
    "print('Number of sentence pairs in the training set: ', len(trainset))\n",
    "print('Number of sentence pairs in the test set: ', len(testset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "The next cell loads the dataset and processes it using a collate function. The collate function is responsible for processing and organizing the input data into batches that can be fed into the neural network for training.\n",
    "\n",
    "The RNN collate function performs several important tasks, including padding the sequences to ensure that they are of equal length, sorting the sequences by length to optimize the training process, and converting the sequences into tensors that can be processed by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=RNNTools.collate, pin_memory=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=False, collate_fn=RNNTools.collate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder architecture using RNN/GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4489, 256)\n",
       "    (gru): GRU(256, 256)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2925, 256)\n",
       "    (gru): GRU(256, 256)\n",
       "    (out): Linear(in_features=256, out_features=2925, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(trainset.input_lang.n_words, trainset.output_lang.n_words, embed_size=256, hidden_size=256)\n",
    "rnn.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The RNN model is optimized using the Adam optimizer and the negative log-likelihood loss (NLLLoss).\n",
    "\n",
    "During training, the RNN model also uses a technique called teacher forcing, which involves feeding the correct previous word in the target sequence to the decoder as input, instead of using the predicted word from the previous time step.\n",
    "\n",
    "Here, teacher forcing is used 50% of the time. This helps the model learn to generate translations more accurately by giving it access to the ground truth translations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    PADDING_VALUE = 0 \n",
    "    teacher_forcing_ratio = 0.5\n",
    "    num_epochs = 2\n",
    "\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)    \n",
    "    criterion = nn.NLLLoss(ignore_index=PADDING_VALUE)\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_data = 0\n",
    "        for src_seqs, src_seq_lengths, tgt_seqs in trainloader:\n",
    "            src_seqs, tgt_seqs = src_seqs.to(device), tgt_seqs.to(device)\n",
    "            \n",
    "            if torch.rand(1) < teacher_forcing_ratio:\n",
    "                teacher_forcing=True\n",
    "            else:\n",
    "                teacher_forcing=False\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = rnn(src_seqs, tgt_seqs, src_seq_lengths, teacher_forcing)\n",
    "            loss = criterion(outputs.permute(0, 2, 1).to(device), tgt_seqs)\n",
    "            \n",
    "            # compute loss metric\n",
    "            total_loss += (loss.item() * src_seqs.shape[1])\n",
    "            total_data += src_seqs.shape[1]\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch: {0} training loss: {1:.3f}\".format(epoch, total_loss/total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    torch.save(rnn.state_dict(), rnn_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model loaded from: models/rnn.pth\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    rnn.load_state_dict(torch.load(rnn_model_save_path, map_location=lambda storage, loc: storage))\n",
    "    print('RNN model loaded from: {}'.format(rnn_model_save_path))\n",
    "    rnn.to(device)\n",
    "    rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnntools = RNNTools(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dataframe where we will save our results for better display formatting\n",
    "results_df = pd.DataFrame(index=range(20), columns=['batch_i', 'Source', 'Actual Translation', 'RNN Translation', 'Transformer Translation'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The table below shows RNN- translations of randomly sampled sentences from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | Source                                      | Actual Translation                | RNN Translation                   |\n",
       "|---:|:--------------------------------------------|:----------------------------------|:----------------------------------|\n",
       "|  0 | je suis en train de boire une biere .       | i m drinking a beer .             | i am drinking a letter .          |\n",
       "|  1 | elles cherchent un bouc emissaire .         | they re looking for a scapegoat . | they re looking for a scapegoat . |\n",
       "|  2 | ils ne constituent pas une menace .         | they re not a threat .            | they re not a bad good . .        |\n",
       "|  3 | tu mens n est ce pas ?                      | you re lying aren t you ?         | you re staying aren t you ?       |\n",
       "|  4 | je suis heureux de vous avoir invitee .     | i m glad i invited you .          | i m glad i invited you .          |\n",
       "|  5 | il connait le maire .                       | he is acquainted with the mayor . | he s open a chinese .             |\n",
       "|  6 | je suis interesse .                         | i m interested .                  | i m not .                         |\n",
       "|  7 | nous sommes amoureux .                      | we re in love .                   | we re in .                        |\n",
       "|  8 | elles sont chretiennes .                    | they are christians .             | they are christians .             |\n",
       "|  9 | je crains que tu m aies mal compris .       | i m afraid you misunderstood me . | i m afraid that will be happy .   |\n",
       "| 10 | on a vraiment besoin d eau .                | we are badly in want of water .   | we re really proud of this .      |\n",
       "| 11 | je suis toujours heureux .                  | i m always happy .                | i m always happy .                |\n",
       "| 12 | tu es tout ce que j ai .                    | you re all i ve got .             | you re all i ve got .             |\n",
       "| 13 | je mange un sandwich .                      | i m eating a sandwich .           | i am eating a sandwich .          |\n",
       "| 14 | il est trop sensible .                      | he is too sensitive .             | he s too drunk .                  |\n",
       "| 15 | vous etes prevenant .                       | you re considerate .              | you re considerate .              |\n",
       "| 16 | j y vais .                                  | i m going .                       | i m going going .                 |\n",
       "| 17 | on n est jamais trop vieux pour apprendre . | you re never too old to learn .   | he s too old to learn too old .   |\n",
       "| 18 | elle prepare le dejeuner .                  | she is making dinner .            | she s missed to the . .           |\n",
       "| 19 | nous sommes tous en train de diner .        | we re all having lunch .          | we re all in . .                  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "for src_seqs, src_mask, tgt_seqs in testloader:\n",
    "    if i >= 20:\n",
    "        break\n",
    "\n",
    "    out_seqs = rnntools.translate(rnn, src_seqs, src_mask)\n",
    "\n",
    "    for r in random.sample(range(0, 64), 1):\n",
    "        results_df.loc[i, 'batch_i'] = r\n",
    "        results_df.loc[i, 'Source'] = rnntools.seq_to_string(src_seqs[:,r], testset.input_lang)\n",
    "        results_df.loc[i, 'Actual Translation'] = rnntools.seq_to_string(tgt_seqs[:,r], testset.output_lang)\n",
    "        results_df.loc[i, 'RNN Translation'] = rnntools.seq_to_string(out_seqs[:,r], testset.output_lang)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "display(Markdown(results_df[['Source', 'Actual Translation', 'RNN Translation']].to_markdown()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the BLEU score for the RNN translation, which ranges from 0 to 100 and is a metric used to measure the similarity between the machine-generated translation and the reference translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 96.69817090034485\n",
      "BLEU score on test data: 47.73730933666229\n"
     ]
    }
   ],
   "source": [
    "score = rnntools.compute_bleu_score(rnn, trainloader, trainset.output_lang)\n",
    "print(f'BLEU score on training data: {score*100}')\n",
    "score = rnntools.compute_bleu_score(rnn, testloader, trainset.output_lang)\n",
    "print(f'BLEU score on test data: {score*100}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also set skip_training for Transformers different from the RNN\n",
    "# skip_training = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN, we first load the dataset and process it with collate function.\n",
    "\n",
    "The Transformers' collate function takes in a batch of input sequences of varying lengths, pads them to the maximum length in the batch, and creates attention masks to indicate the padding locations. The collate function also creates a batch of target sequences by shifting the input sequences by one time step and adding a start-of-sequence token at the beginning of each target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=TransformersTools.collate, pin_memory=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=False, collate_fn=TransformersTools.collate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder architecture using Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformers(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4489, 256, padding_idx=0)\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-2): 3 x EncoderBlock(\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2925, 256, padding_idx=0)\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-2): 3 x DecoderBlock(\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=2925, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra = Transformers(trainset.input_lang.n_words, trainset.output_lang.n_words, n_blocks=3, n_features=256, n_heads=16, n_hidden=1024)\n",
    "tra.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Transformers is also trained with Adam optimizer and NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    PADDING_VALUE = 0\n",
    "    num_epochs = 2\n",
    "\n",
    "    optimizer = torch.optim.Adam(tra.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss(ignore_index=PADDING_VALUE)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_data = 0\n",
    "        for src_seqs, src_mask, tgt_seqs in trainloader:\n",
    "            src_seqs, src_mask, tgt_seqs = src_seqs.to(device), src_mask.to(device), tgt_seqs.to(device)\n",
    "            \n",
    "            # forward\n",
    "            outputs = tra(src_seqs, tgt_seqs, src_mask)\n",
    "            \n",
    "            # compute loss metric\n",
    "            loss = criterion(outputs.permute(0, 2, 1).to(device), tgt_seqs[1:])\n",
    "            total_loss += (loss.item() * src_seqs.shape[1])\n",
    "            total_data += src_seqs.shape[1]\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch: {0} training loss: {1:.3f}\".format(epoch, total_loss/total_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    torch.save(tra.state_dict(), tra_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers model loaded from: models/transformers.pth\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    tra.load_state_dict(torch.load(tra_model_save_path, map_location=lambda storage, loc: storage))\n",
    "    print('Transformers model loaded from: {}'.format(tra_model_save_path))\n",
    "    tra.to(device)\n",
    "    tra.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tratools = TransformersTools(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The table below shows Transformers- translations of the same batch of test sentences as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | Source                                      | Actual Translation                | Transformer Translation           |\n",
       "|---:|:--------------------------------------------|:----------------------------------|:----------------------------------|\n",
       "|  0 | je suis en train de boire une biere .       | i m drinking a beer .             | i am drinking a beer .            |\n",
       "|  1 | elles cherchent un bouc emissaire .         | they re looking for a scapegoat . | they re looking for a scapegoat . |\n",
       "|  2 | ils ne constituent pas une menace .         | they re not a threat .            | they re not watching .            |\n",
       "|  3 | tu mens n est ce pas ?                      | you re lying aren t you ?         | you re lying aren t you ?         |\n",
       "|  4 | je suis heureux de vous avoir invitee .     | i m glad i invited you .          | i m glad i invited you .          |\n",
       "|  5 | il connait le maire .                       | he is acquainted with the mayor . | he s stalling for tea .           |\n",
       "|  6 | je suis interesse .                         | i m interested .                  | i m interested .                  |\n",
       "|  7 | nous sommes amoureux .                      | we re in love .                   | we re biased .                    |\n",
       "|  8 | elles sont chretiennes .                    | they are christians .             | they are christians .             |\n",
       "|  9 | je crains que tu m aies mal compris .       | i m afraid you misunderstood me . | i m afraid you will get may .     |\n",
       "| 10 | on a vraiment besoin d eau .                | we are badly in want of water .   | we re truly need .                |\n",
       "| 11 | je suis toujours heureux .                  | i m always happy .                | i m always happy .                |\n",
       "| 12 | tu es tout ce que j ai .                    | you re all i ve got .             | you re all i ve got .             |\n",
       "| 13 | je mange un sandwich .                      | i m eating a sandwich .           | i m eating a sandwich .           |\n",
       "| 14 | il est trop sensible .                      | he is too sensitive .             | he s too sensitive .              |\n",
       "| 15 | vous etes prevenant .                       | you re considerate .              | you re considerate .              |\n",
       "| 16 | j y vais .                                  | i m going .                       | i m going there .                 |\n",
       "| 17 | on n est jamais trop vieux pour apprendre . | you re never too old to learn .   | you are too old to learn .        |\n",
       "| 18 | elle prepare le dejeuner .                  | she is making dinner .            | she is making dinner .            |\n",
       "| 19 | nous sommes tous en train de diner .        | we re all having lunch .          | we re all growing .               |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "for src_seqs, src_mask, tgt_seqs in testloader:\n",
    "    if i >= 20:\n",
    "        break\n",
    "\n",
    "    out_seqs = tratools.translate(tra, src_seqs, src_mask)\n",
    "\n",
    "    r = results_df.loc[i, 'batch_i']\n",
    "    results_df.loc[i, 'Transformer Translation'] = tratools.seq_to_string(out_seqs[:,r], testset.output_lang)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "display(Markdown(results_df[['Source', 'Actual Translation', 'Transformer Translation']].to_markdown()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the BLEU score for the Transformers translation. It performed better than the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 92.83134043323167\n",
      "BLEU score on test data: 58.79185315508608\n"
     ]
    }
   ],
   "source": [
    "score = tratools.compute_bleu_score(tra, trainloader, trainset.output_lang)\n",
    "print(f'BLEU score on training data: {score*100}')\n",
    "score = tratools.compute_bleu_score(tra, testloader, trainset.output_lang)\n",
    "print(f'BLEU score on test data: {score*100}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN vs. Transformers: Combined Results\n",
    "\n",
    "Here are the same sentences shown side by side for better comparison. It can be observed that the Transformers model has produced more accurate translations in this sample, which is consistent with the BLEU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Source                                      | Actual Translation                | RNN Translation                   | Transformer Translation           |\n",
       "|---:|:--------------------------------------------|:----------------------------------|:----------------------------------|:----------------------------------|\n",
       "|  0 | je suis en train de boire une biere .       | i m drinking a beer .             | i am drinking a letter .          | i am drinking a beer .            |\n",
       "|  1 | elles cherchent un bouc emissaire .         | they re looking for a scapegoat . | they re looking for a scapegoat . | they re looking for a scapegoat . |\n",
       "|  2 | ils ne constituent pas une menace .         | they re not a threat .            | they re not a bad good . .        | they re not watching .            |\n",
       "|  3 | tu mens n est ce pas ?                      | you re lying aren t you ?         | you re staying aren t you ?       | you re lying aren t you ?         |\n",
       "|  4 | je suis heureux de vous avoir invitee .     | i m glad i invited you .          | i m glad i invited you .          | i m glad i invited you .          |\n",
       "|  5 | il connait le maire .                       | he is acquainted with the mayor . | he s open a chinese .             | he s stalling for tea .           |\n",
       "|  6 | je suis interesse .                         | i m interested .                  | i m not .                         | i m interested .                  |\n",
       "|  7 | nous sommes amoureux .                      | we re in love .                   | we re in .                        | we re biased .                    |\n",
       "|  8 | elles sont chretiennes .                    | they are christians .             | they are christians .             | they are christians .             |\n",
       "|  9 | je crains que tu m aies mal compris .       | i m afraid you misunderstood me . | i m afraid that will be happy .   | i m afraid you will get may .     |\n",
       "| 10 | on a vraiment besoin d eau .                | we are badly in want of water .   | we re really proud of this .      | we re truly need .                |\n",
       "| 11 | je suis toujours heureux .                  | i m always happy .                | i m always happy .                | i m always happy .                |\n",
       "| 12 | tu es tout ce que j ai .                    | you re all i ve got .             | you re all i ve got .             | you re all i ve got .             |\n",
       "| 13 | je mange un sandwich .                      | i m eating a sandwich .           | i am eating a sandwich .          | i m eating a sandwich .           |\n",
       "| 14 | il est trop sensible .                      | he is too sensitive .             | he s too drunk .                  | he s too sensitive .              |\n",
       "| 15 | vous etes prevenant .                       | you re considerate .              | you re considerate .              | you re considerate .              |\n",
       "| 16 | j y vais .                                  | i m going .                       | i m going going .                 | i m going there .                 |\n",
       "| 17 | on n est jamais trop vieux pour apprendre . | you re never too old to learn .   | he s too old to learn too old .   | you are too old to learn .        |\n",
       "| 18 | elle prepare le dejeuner .                  | she is making dinner .            | she s missed to the . .           | she is making dinner .            |\n",
       "| 19 | nous sommes tous en train de diner .        | we re all having lunch .          | we re all in . .                  | we re all growing .               |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_df[['Source', 'Actual Translation', 'RNN Translation', 'Transformer Translation']].to_markdown()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_projs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
